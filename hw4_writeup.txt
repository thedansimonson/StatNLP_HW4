Dan Simonson
StatNLP Homework 4
E Graham Katz
13 March 2012

#############
# Problem 1 #
#############
Files:
hw4_01_delta.jpg, hw4_01_delta2.jpg
	My Viterbi table and POS tagging.
hw4_01.py
	The script used to generate the Transition and Emission probability tables below.
tagseq.txt
	The tag sequence used by hw4_01.py

Outputs:
   Transition Probabilities (column given row, so P(DET|V)=0.636)
                 <S>       ADJ       DET         N        PN      PREP         V
       <S>       0.0       0.0   0.55556       0.0   0.44444       0.0       0.0
       ADJ       0.0       0.0       0.0       1.0       0.0       0.0       0.0
       DET       0.0   0.11111       0.0   0.88889       0.0       0.0       0.0
         N       0.2       0.0       0.0       0.0      0.05       0.5       0.3
        PN   0.28571       0.0       0.0   0.07143       0.0   0.28571   0.35714
      PREP       0.0       0.0   0.46667       0.0   0.53333       0.0       0.0
         V   0.09091       0.0   0.63636       0.0   0.09091   0.09091       0.0

   Emission Probabilities (column given row)
                   a        anbinoculars     books   buffalo  catwoman   clothes
       ADJ       0.0       0.0       0.0       0.0       0.0       0.0       0.0
       DET   0.69231   0.07692       0.0       0.0       0.0       0.0       0.0
         N       0.0       0.0       0.0       0.0       0.2       0.0       0.1
        PN       0.0       0.0       0.3       0.0       0.3       0.4       0.0
      PREP       0.0       0.0       0.0       0.0       0.0       0.0       0.0
         V       0.0       0.0       0.0      0.25     0.125       0.0       0.0

                dust    kissed       man        on      open     patty       saw
       ADJ       0.0       0.0       0.0       0.0       1.0       0.0       0.0
       DET       0.0       0.0       0.0       0.0       0.0       0.0       0.0
         N       0.1       0.0       0.6       0.0       0.0       0.0       0.3
        PN       0.0       0.0       0.0       0.0       0.0       0.1       0.0
      PREP       0.0       0.0       0.0       0.5       0.0       0.0       0.0
         V       0.0     0.125       0.0       0.0     0.125       0.0      0.75

           spiderman    sticks       the      tree      with     woman
       ADJ       0.0       0.0       0.0       0.0       0.0       0.0
       DET       0.0       0.0   0.69231       0.0       0.0       0.0
         N       0.0       0.1       0.0       0.4       0.0       0.4
        PN       0.3       0.0       0.0       0.0       0.0       0.0
      PREP       0.0       0.0       0.0       0.0      0.75       0.0
         V       0.0       0.0       0.0       0.0       0.0       0.0

The delta tables and calculations to derive them are available in 
hw4_01_delta.jpg and hw4_01_delta2.jpg . The result for the "buffalo 
buffalo..." parse: PN V PN V PN. 

Note that I made a mistake on the original calculation of the table. 
However, luckily, I merely wrote the result for PN in the N row, so 
assuming that change produces the correct result. As visible in the
calculation tables, the error was made in duplication, not calculation.

#############
# Problem 2 #
#############
Files:
hw4_02_findingBigOTime.ods
	Uses the durations measured in hw4_02_speedEstimate.py to estimate the Big-O time
	of the parser generated by hw4_02.py.
hw4_02_precisionRecall.py
	Uses the sentences in /hw4_vitParses/ to measure the precision/recall of the parser.
hw4_02_precisionRecall_parser.py
	Parses sentences for precisionRecall.py. Saves every sentence after parsing so that
	parsing can resume later and/or apply them in the precision/recall calcuations.
hw4_02_run.py
	This is a small program that parses single, user-input sentences. Intended for testing
	purposes. If you use this , be sure that you 're putting spaces between everything 
	like this well-tokenized sentence .
hw4_02_speedEstimate.py
	Measures the time it takes to parse an ever-increasingly long set of sentences. 
hw4_02_speedEstimate_part2.py
	Estimates, based on the data collected in hw4_02_speedEstimate.py, the length of time
	it would take to parse the entire test sentence set.
hw4_02_unittest.py
	An early attempt at evaluating the parser.
hw4_02.py
	Generates a parser; saves it to hw4_vitParser.pkl
POS/
	A directory from the PTB provided for this assignment. Used to add the lexicon to the 
	parser. Skipped because of the large size.

Outputs:
hw4_testTrees.pkl
	The 10% of the PTB excluded for testing purposes.
hw4_vitParser.pkl
	The saved version of the parser.
hw4_vitParses/
	Parses generated by hw4_02_precisionRecall_parser.py  

Write-up:
I'm disappointed with how slow this thing is. Most of my code hasn't been to handle the tasks 
at hand but to deal with issues regarding performance and to accept the reality of the slow
parser. There are three main parts here:
	hw4_02.py creates the parser--loading the Treebank, extra lexical items, filtering and
		modifying appropriately, then storing in a pickled file: hw4_vitParser.pkl. 
	hw4_02_precisionRecall_parser.py uses the parser provided to create pickled parses of 
		the sentences in hw4_testTrees.pkl.
	hw4_02_precisionRecall.py calculates the precision and recall of the sentences based on
		the parses pickled *_parser.py.

However, as to be expected, this didn't work out so friendly. My shorter sentences would 
parse, but as they grew longer, the sentences never seemed to finish. To figure out if 
sentence length corresponded to parse duration--and to what extent that it did--I wrote
hw4_02_speedEstimate.py. It has 15 sentences that are lexically similar, grammatical in 
English, and of increasing length. 

The results were suprising. I plotted the results on paper as they were displayed; initially
the growth looked linear. By the time I was near done, the growth started to look exponential.
hw4_02_findingBigOTime.ods verified that through an exponential least squares fit between
the points with a high R^2 value. With the function provided by the spreadsheet, I estimated
the time necessary to parse the entire test set. Including all sentences, a complete parse
would require 6600 /years/. This seems absurdly long to me, but this is far longer than the 
first guess I had of about 40 hours of computer time. 

As a result, I've given up on improving this for now. The sample of Python code provided in 
the assignment used the Viterbi parser built into NLTK. I'm not sure what else to change in
generating the parser except to write my own, and that's a project I don't feel like jumping 
into now. I also doubt I could do a better job, in the short term, than the folks who wrote
NLTK, so writing my own parser doesn't seem to be a profitable venture. 

hw4_02_precisionRecall_parser.py generates each parse and saves it in a file in hw4_vitParses/.
I generated two parses in order and a couple in order of sentence length. A lot of these are 
incredibly short and don't parse correctly since they're actually sentence fragments and not
complete sentences. 

hw4_02_precisionRecall.py uses a variant of the script provided by Stephen for measuring 
precision and recall. Of the sentences I attempted to parse, 47 successfully did without an 
error--for some reason, many cardinal numbers weren't covered appropriately and other issues
occurred on the technical end. The script evaluated the suviving parses as follows:
Average precision: 0.670188971785
Average recall: 0.585026823059

#############
# Problem 3 #
#############
The sums at the bottom fit into three groups. The first group (columns J and K) represents the 
expected number of days we were in state C or H. There are 33 days, so these add up to 33. 

The second group represents the joint probabilities of both reaching C or H and eating 1, 2, 
or 3 ice creams at the end of a day. There are six of these (columns L,M,N,O,P,Q) to represent 
all the combinations of our possibilities. These, again, add up to 33 days for all the days ice 
cream was eaten. 

The third group contains transition probabilities given a particular state. 
There are four possible transitions (C to C), (H to H), (C to H), and (H to C).
Their sum is 32 since there are only 32 transitions between 33 states.

Our emissions estimations emerge from simple realizations of the formula:
	P(X|Y) = P(X & Y) / P(Y)
So, for example, P(1|C) = L60/J60 = P(C & 1) / P(C) 

These new estimations of the probabilities are reintroduced in the same way that the initial
parameters were applied; that is, the values of a(C), a(H), b(C), and b(H) are recalculated 
based on the new parameters. This process is this repeated until a satisfactory level of 
precision has been reached.
