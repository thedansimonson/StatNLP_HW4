Dan Simonson
StatNLP Homework 4
E Graham Katz
13 March 2012

#############
# Problem 1 #
#############
Files:
hw4_01_delta.jpg, hw4_01_delta2.jpg
	My Viterbi table and POS tagging.
hw4_01.py
	The script used to generate the Transition and Emission probability tables below.
tagseq.txt
	The tag sequence used by hw4_01.py

Outputs:
   Transition Probabilities (column given row, so P(DET|V)=0.636)
                 <S>       ADJ       DET         N        PN      PREP         V
       <S>       0.0       0.0   0.55556       0.0   0.44444       0.0       0.0
       ADJ       0.0       0.0       0.0       1.0       0.0       0.0       0.0
       DET       0.0   0.11111       0.0   0.88889       0.0       0.0       0.0
         N       0.2       0.0       0.0       0.0      0.05       0.5       0.3
        PN   0.28571       0.0       0.0   0.07143       0.0   0.28571   0.35714
      PREP       0.0       0.0   0.46667       0.0   0.53333       0.0       0.0
         V   0.09091       0.0   0.63636       0.0   0.09091   0.09091       0.0

   Emission Probabilities (column given row)
                   a        anbinoculars     books   buffalo  catwoman   clothes
       ADJ       0.0       0.0       0.0       0.0       0.0       0.0       0.0
       DET   0.69231   0.07692       0.0       0.0       0.0       0.0       0.0
         N       0.0       0.0       0.0       0.0       0.2       0.0       0.1
        PN       0.0       0.0       0.3       0.0       0.3       0.4       0.0
      PREP       0.0       0.0       0.0       0.0       0.0       0.0       0.0
         V       0.0       0.0       0.0      0.25     0.125       0.0       0.0

                dust    kissed       man        on      open     patty       saw
       ADJ       0.0       0.0       0.0       0.0       1.0       0.0       0.0
       DET       0.0       0.0       0.0       0.0       0.0       0.0       0.0
         N       0.1       0.0       0.6       0.0       0.0       0.0       0.3
        PN       0.0       0.0       0.0       0.0       0.0       0.1       0.0
      PREP       0.0       0.0       0.0       0.5       0.0       0.0       0.0
         V       0.0     0.125       0.0       0.0     0.125       0.0      0.75

           spiderman    sticks       the      tree      with     woman
       ADJ       0.0       0.0       0.0       0.0       0.0       0.0
       DET       0.0       0.0   0.69231       0.0       0.0       0.0
         N       0.0       0.1       0.0       0.4       0.0       0.4
        PN       0.3       0.0       0.0       0.0       0.0       0.0
      PREP       0.0       0.0       0.0       0.0      0.75       0.0
         V       0.0       0.0       0.0       0.0       0.0       0.0

The delta tables and calculations to derive them are available in 
hw4_01_delta.jpg and hw4_01_delta2.jpg . The result for the "buffalo 
buffalo..." parse: PN V PN V PN. 

Note that I made a mistake on the original calculation of the table. 
However, luckily, I merely wrote the result for PN in the N row, so 
assuming that change produces the correct result. As visible in the
calculation tables, the error was made in duplication, not calculation.

#############
# Problem 2 #
#############
Files:
hw4_02_precisionRecall.py
	This is a precision/recall measure program of the parser.
hw4_02_precisionRecall_parser.py
	Parses sentences for precisionRecall.py. Saves every sentence after parsing so that
	parsing can resume later and/or apply them in the precision/recall calcuations.
hw4_02_run.py
	This is a small program that parses single, user-input sentences. Intended for testing
	purposes. If you use this , be sure that you 're putting spaces between everything 
	like this well-tokenized sentence .
hw4_02_unittest.py
	An early attempt at evaluating the parser.
hw4_02.py
	Generates a parser; saves it to hw4_vitParser.pkl
hw4_testTrees.pkl
	The 10% of the PTB excluded for testing purposes.
hw4_vitParser.pkl
	The saved version of the parser.
/POS/
	A directory from the PTB provided for this assignment. Used to add the lexicon to the 
	parser. Skipped because of the large size.

Outputs:

Write-up:
I'm disappointed with how slow this thing is. Most of my code hasn't been to handle the tasks 
at hand but to deal with issues regarding performance and to accept the reality of the slow
parser. There are three main parts here:
	hw4_02.py creates the parser--loading the Treebank, extra lexical items, filtering and
		modifying appropriately, then storing in a pickled file: hw4_vitParser.pkl. 
	hw4_02_precisionRecall_parser.py uses the parser provided to create pickled parses of 
		the sentences in hw4_testTrees.pkl.
	hw4_02_precisionRecall.py calculates the precision and recall of the sentences based on
		the parses pickled *_parser.py.

The results of hw4_02_precisionRecall.py are provided in Outputs above.

To compensate for the slow parsing time, I did some poor-man's cloud computing. Using the 
github repository for assignment, I cloned the necessary data files into all of the severs I 
had access to:
	discverb.com (my personal server)
	armstrong
	The Amazon AWS server for XFST from last semester.
In addition to multiple computers I own:
	My HP Desktop
	My Dell Laptop
	My Macbook Air
The Dell I used to maintain connection to the servers, and I kept it hard-wired to an ethernet
connection just in case. 


I distributed the sentences as follows, based on a quick speed test:


The only baseline I had for parsetime was 20 minutes. I had 397 sentences to parse. This 
translated to roughly 132 hours of computer time, or 22 hours across 6 computers, assuming no
problems. 

#############
# Problem 3 #
#############
The sums at the bottom fit into three groups. The first group (columns J and K) represents the 
expected number of days we were in state C or H. There are 33 days, so these add up to 33. 

The second group represents the joint probabilities of both reaching C or H and eating 1, 2, 
or 3 ice creams at the end of a day. There are six of these (columns L,M,N,O,P,Q) to represent 
all the combinations of our possibilities. These, again, add up to 33 days for all the days ice 
cream was eaten. 

The third group contains transition probabilities given a particular state. 
There are four possible transitions (C to C), (H to H), (C to H), and (H to C).
Their sum is 32 since there are only 32 transitions between 33 states.

Our emissions estimations emerge from simple realizations of the formula:
	P(X|Y) = P(X & Y) / P(Y)
So, for example, P(1|C) = L60/J60 = P(C & 1) / P(C) 

These new estimations of the probabilities are reintroduced in the same way that the initial
parameters were applied; that is, the values of a(C), a(H), b(C), and b(H) are recalculated 
based on the new parameters. This process is this repeated until a satisfactory level of 
precision has been reached.
